{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484e7614",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c0ec7",
   "metadata": {},
   "source": [
    "사전학습된 CLM(Causal Language Model)을 이용하여 자연어 문장을 생성하는 방법에 대해 살펴 보겠습니다.  \n",
    "GPT2-XL은 GPT2의 1.5B 파라미터 버전으로 트랜스포머 기반의 CLM 입니다.  \n",
    "Greedy Search Decoding, Beam Search Decoding, Random Sampling, Top-K/Top-P Sampling 방법 등을 실습합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6686293",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b492240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a65e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user transformers==4.38.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a24d27",
   "metadata": {},
   "source": [
    "### **GPT2-XL**: 48-layer, 1600-hidden, 25-heads, 1,558M parameters, English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d65463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"gpt2-xl\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 사전학습 모델 'gpt2-xl'에 사용된 Tokenizer 가져옵니다.\n",
    "# Causl LM 'gpt2-xl'을 device로 가져옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc3eaf",
   "metadata": {},
   "source": [
    "## 1. Greedy Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc3e37",
   "metadata": {},
   "source": [
    "가장 간단한 디코딩 방식은 각 타임스텝에서 가장 확률이 높은 토큰만을 선택하는 방법입니다.  \n",
    "Generate() 함수가 있지만, LLM을 이용한 텍스트 생성 과정을 이해하기 위해 직접 구현해 보겠습니다.  \n",
    "매 타임스텝마다 마지막 토큰에 대한 Logit을 선택하고, SoftMax를 통해 확률값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # 첫번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용합니다.\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # 가장 높은 확률의 토큰을 저장합니다.\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob: .2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # 예측한 다음 토큰을 입력에 추가합니다.    \n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb514c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most ( 8.53%)</td>\n",
       "      <td>only ( 4.96%)</td>\n",
       "      <td>best ( 4.65%)</td>\n",
       "      <td>Transformers ( 4.37%)</td>\n",
       "      <td>ultimate ( 2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular ( 16.78%)</td>\n",
       "      <td>powerful ( 5.37%)</td>\n",
       "      <td>common ( 4.96%)</td>\n",
       "      <td>famous ( 3.72%)</td>\n",
       "      <td>successful ( 3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy ( 10.63%)</td>\n",
       "      <td>toys ( 7.23%)</td>\n",
       "      <td>Transformers ( 6.60%)</td>\n",
       "      <td>of ( 5.46%)</td>\n",
       "      <td>and ( 3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line ( 34.38%)</td>\n",
       "      <td>in ( 18.20%)</td>\n",
       "      <td>of ( 11.71%)</td>\n",
       "      <td>brand ( 6.10%)</td>\n",
       "      <td>line ( 2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in ( 46.28%)</td>\n",
       "      <td>of ( 15.09%)</td>\n",
       "      <td>, ( 4.94%)</td>\n",
       "      <td>on ( 4.40%)</td>\n",
       "      <td>ever ( 2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the ( 65.99%)</td>\n",
       "      <td>history ( 12.42%)</td>\n",
       "      <td>America ( 6.91%)</td>\n",
       "      <td>Japan ( 2.44%)</td>\n",
       "      <td>North ( 1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world ( 69.26%)</td>\n",
       "      <td>United ( 4.55%)</td>\n",
       "      <td>history ( 4.29%)</td>\n",
       "      <td>US ( 4.23%)</td>\n",
       "      <td>U ( 2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, ( 39.73%)</td>\n",
       "      <td>. ( 30.64%)</td>\n",
       "      <td>and ( 9.87%)</td>\n",
       "      <td>with ( 2.32%)</td>\n",
       "      <td>today ( 1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input            Choice 1  \\\n",
       "0                               Transformers are the       most ( 8.53%)   \n",
       "1                          Transformers are the most   popular ( 16.78%)   \n",
       "2                  Transformers are the most popular       toy ( 10.63%)   \n",
       "3              Transformers are the most popular toy      line ( 34.38%)   \n",
       "4         Transformers are the most popular toy line        in ( 46.28%)   \n",
       "5      Transformers are the most popular toy line in       the ( 65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world ( 69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , ( 39.73%)   \n",
       "\n",
       "             Choice 2                Choice 3                Choice 4  \\\n",
       "0       only ( 4.96%)           best ( 4.65%)   Transformers ( 4.37%)   \n",
       "1   powerful ( 5.37%)         common ( 4.96%)         famous ( 3.72%)   \n",
       "2       toys ( 7.23%)   Transformers ( 6.60%)             of ( 5.46%)   \n",
       "3        in ( 18.20%)            of ( 11.71%)          brand ( 6.10%)   \n",
       "4        of ( 15.09%)              , ( 4.94%)             on ( 4.40%)   \n",
       "5   history ( 12.42%)        America ( 6.91%)          Japan ( 2.44%)   \n",
       "6     United ( 4.55%)        history ( 4.29%)             US ( 4.23%)   \n",
       "7         . ( 30.64%)            and ( 9.87%)           with ( 2.32%)   \n",
       "\n",
       "               Choice 5  \n",
       "0     ultimate ( 2.16%)  \n",
       "1   successful ( 3.20%)  \n",
       "2          and ( 3.76%)  \n",
       "3         line ( 2.69%)  \n",
       "4         ever ( 2.72%)  \n",
       "5        North ( 1.40%)  \n",
       "6            U ( 2.30%)  \n",
       "7        today ( 1.74%)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54999129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "max_length = 12\n",
    "\n",
    "encoded_input = tokenizer(input_txt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# 입력에 대한 Greedy Search: max_length, do_sample 파라미터를 설정합니다.\n",
    "output_greedy = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9e857",
   "metadata": {},
   "source": [
    "## 2. Beam Search Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cbbdc",
   "metadata": {},
   "source": [
    "- **log_probs_from_logits()**: 하나의 토큰에 대한 로그 확률을 제공합니다.\n",
    "- **sequence_logprob()**: 시퀀스에 대한 전체 로그 확률값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83f1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a895abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633bfa5",
   "metadata": {},
   "source": [
    "빔서치 디코딩은 확률이 가장 높은 상위 num_beam 갯수 만큼의 다음 토큰 시퀀스를 추적합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8218ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "encoded_input = tokenizer(input_txt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d06f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "\n",
      "Log Probability: -133.66\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 입력에 대한 Beam Search: max_length, num_beam, do_sample 파라미터 등을 설정하세요.\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd5966",
   "metadata": {},
   "source": [
    "- **no_repeat_ngram_size**: 동일한 텍스트가 반복 생성되는 문제를 해결하기 위하여 n-gram penalty를 부과할 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f5d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "\n",
      "Log Probability: -171.56\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 이전 Beam Search 결과에 no_repeat_ngram_size 설정 추가\n",
    "output_beam = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nLog Probability: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af2c04",
   "metadata": {},
   "source": [
    "## 3. Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c7ccc",
   "metadata": {},
   "source": [
    "각 타임스텝 내 모델이 출력한 전체 어휘사전에 대한 확률분포에서 랜덤하게 샘플링하는 방법입니다.  \n",
    "- **temperature**: 소프트맥스 함수를 적용하기 전에 로짓의 스케일을 조정하는 Temperature 파라미터를 추가하면 출력의 다양성을 제어할 수 있습니다.  \n",
    "T << 1 일때 낮은 확률의 토큰들을 억제하며, T >> 1 일때는 분포가 평평해져서 각 토큰의 확률들이 동일해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a242a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Scientists discovered that Unicarni-hyrde unicorns are now very comfortable moving from place-to-\"place,\" with no need for their mothers on weekends for their protection of breeding spots. To protect their calves which they milk at, many times the mother Unicorn leads them at an astonishing 24 mph across a valley up from one side at full moon to another at full moon so her baby unicorn could\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Random Sampleing 방법을 통한 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=2.0)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50f7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns have been living in the valley for at least 150 years. They are estimated to number between 100 and 200.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns have been living in the valley for at least 150 years. They are estimated to number between 100 and 200.\n",
      "\n",
      "The researchers believe that the unicorns have been living in the valley for at least 150 years.\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# Temperature를 변경하여 문장 생성: max_length, do_sample, temperature 파라미터를 설정하세요.\n",
    "output_temp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, temperature=0.5)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913df480",
   "metadata": {},
   "source": [
    "## 4. Top-K and Top-P Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c192b2",
   "metadata": {},
   "source": [
    "두 방법 모두 샘플링에 사용할 토큰의 갯수를 줄인다는 개념에 기초하고 있습니다.  \n",
    "\n",
    "- Top-K 샘플링: 확률이 가장 높은 K개 토큰에서만 샘플링하고 확률이 낮은 토큰을 제외함으로써,\n",
    "확률 분포의 롱테일을 잘라내고 확률이 가장 높은 토큰에서만 샘플링하는 방법입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64099b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of a herd herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains proves that they do not necessarily speak a language that is closer to human tongues, research showed.The findings were made by a team of scientists from the US National Park Service conducted in 2012 in a study to further understand the Andes as a natural habitat and a biodiversity hotspot, while\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# Top-K 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topk = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_k=50)\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6448e0",
   "metadata": {},
   "source": [
    "- Top-P 샘플링: 고정된 컷오프 값을 사용하지 않고, 어디서 컷오프할 것인지 확률질량(Probability Mass) 조건을 지정합니다.  \n",
    "모든 토큰을 확률에 따라 내림차순으로 정렬하고, 누적 확률값에 도달할 때까지 토큰들을 하나씩 추가하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e808654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"In the early 1970s, while researching a number of ancient documents from Mexico and Peru, I discovered a few documents relating to the Andean region,\" said Dr. Walter A. Bock, a biologist and co-director of the Institute for Creation Research. \"These documents were written in Mayan, Aztec and pre-Aztec Spanish, and they described ancient Mayan and Aztec cities\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# Top-P 샘플링을 적용하여 문장 생성: max_length, do_sample, top_k 파라미터를 설정하세요.\n",
    "output_topp = model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id, max_length=max_length, do_sample=True, top_p=0.90)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f47ef4",
   "metadata": {},
   "source": [
    "- Ref. Natural Language Processing with Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
