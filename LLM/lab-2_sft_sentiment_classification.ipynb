{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97e3126",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84393ae",
   "metadata": {},
   "source": [
    "텍스트 분류는 자연어처리의 대표적인 태스크로 주어진 텍스트를 여러개 카테고리로 분류하는 기술입니다.  \n",
    "대표적인 예로는 감성 분석(Sentiment Analysis)을 들 수 있습니다.  \n",
    "본 예제에서는 사전학습된 DistilBERT를 활용해서 감성 분석을 파인튜닝하는 과정에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869fc01",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a26cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요 \n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "\n",
    "proxies = {\n",
    "'http': '75.17.107.42:8080',\n",
    "'https': '75.17.107.42:8080'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539502ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958376d",
   "metadata": {},
   "source": [
    "### 1. DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896e253",
   "metadata": {},
   "source": [
    "허깅페이스 허브에서 제공하는 \"emotion\" 데이터셋을 파인튜닝에 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd596b7",
   "metadata": {},
   "source": [
    "Emotion 데이터셋은 Train Set 16,000개, Validation Set 2,000개, Test Set 2,000개로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 다음 코드를 완성하세요!! (\"emotion\" 데이터셋 로드)\n",
    "emotions = load_dataset(\"emotion\")\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f372ba0",
   "metadata": {},
   "source": [
    "Emotion 데이터셋의 레이블은 총 6개 카데고리로 구성되어 있습니다 (Sadness, Joy, Love, Anger, Fear, Surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4371e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1644942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51cdde2",
   "metadata": {},
   "source": [
    "### 2. DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48dd0d3",
   "metadata": {},
   "source": [
    "데이터셋의 포맷과 카테고리별 분포 등에 대해 살펴 보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d774078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ab3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False,\n",
    "           showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25cd0f",
   "metadata": {},
   "source": [
    "DataFrame 포맷으로부터 데이터셋 출력 포맷 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee7ee7",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14980d",
   "metadata": {},
   "source": [
    "텍스트를 토큰으로 분할하고 각 토큰을 정수로 매핑합니다.\n",
    "\\[CLS\\]와 \\[SEP\\]는 시퀀스의 시작과 끝을 의미하며, ##IZING와 같이 '##'은 앞 토큰과 공백으로 분리된 것이 아니라, \n",
    "앞의 토큰과 연결된 단어였음을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "# 다음 코드를 완성하세요!! (사전학습 모델에 사용된 Tokenizer 가져오기)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05758abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534c4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb086367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608eac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589199b",
   "metadata": {},
   "source": [
    "Fine-Tuning 학습 데이터를 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a248ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34211487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 코드를 완성하세요!! (Dataset.map Method를 이용하여 emotions 데이터셋의 텍스트를 토크화)\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78c56e",
   "metadata": {},
   "source": [
    "### 4. Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78568e73",
   "metadata": {},
   "source": [
    "텍스트 분류 학습을 위해 사전학습된 **DistilBERT** 기반으로 **SquenceClassification** 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bd3dd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "# 다음 코드를 완성하세요!! (사전학습 모델을 기반으로 Sequence Classification 모델 구성)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e454076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "# 다음 코드를 완성하세요!! (학습 파라미터 설정: num_train_epochs)\n",
    "training_args = TrainingArguments(output_dir=\"test-trainer\",\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# 다음 코드를 완성하세요!! (Trainer 설정: model, args, tokenizer)\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbed030",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e549a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./test-trainer/best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1cf2d",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1921f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_text = \"I saw a movie today and it was really good.\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(**inputs)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Predicted class index: {predicted_class_idx}\")\n",
    "print(f\"Predicted label: {label_int2str(predicted_class_idx)}\")\n",
    "print(f\"Class probability: {probabilities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
