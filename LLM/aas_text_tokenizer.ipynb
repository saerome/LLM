{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6324095b",
   "metadata": {},
   "source": [
    "# Text Tokenizer\n",
    "\n",
    "자연어처리에 있어 토크나이저는 문장을 작은 단위의 토큰(단어 또는 부분단어)으로 분할하는 역할을 하며, \n",
    "본 실습에서는 대표적인 SubWord Tokenizer 방법들에 대해 살펴보도록 하겠습니다.  \n",
    "- BPE Tokenization (Algorithm) \n",
    "- Pretrained Tokenizer (WordPiece)  \n",
    "- Tokenizer Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a43cad",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df194e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f2f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd10d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user transformers==4.38.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2446900",
   "metadata": {},
   "source": [
    "## 1. BPE Tokenization (Algorithm)\n",
    "\n",
    "BPE 토크나이저는 기본 단위(단일 문자)의 리스트로 시작해서 가장 빈번하게 함께 등장하는 토큰을 합쳐 어휘사전에 새롭게 추가하는 방식으로 원하는 어휘사전 크기에 도달할 때까지 점진적으로 새 토큰 만드는 과정을 반복합니다.  \n",
    "아래 실습에서는 논문(https://arxiv.org/pdf/1508.07909.pdf) 에 언급된 알고리즘을 그대로 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0dc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ce7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_merges = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e81d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'l o w </w>' : 5,\n",
    "         'l o w e r </w>' : 2,\n",
    "         'n e w e s t </w>' : 6,\n",
    "         'w i d e s t </w>' : 3\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a658f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3}\n",
      "New merge: ('e', 's')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3}\n",
      "New merge: ('es', 't')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3}\n",
      "New merge: ('est', '</w>')\n",
      "Vocabulary: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('l', 'o')\n",
      "Vocabulary: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('lo', 'w')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('n', 'e')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('ne', 'w')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('new', 'est</w>')\n",
      "Vocabulary: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('low', '</w>')\n",
      "Vocabulary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequences : {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3}\n",
      "New merge: ('w', 'i')\n",
      "Vocabulary: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(vocab):\n",
    "    # 유니그램의 pair들의 빈도수를 카운트\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    print('Frequences :', dict(pairs))\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "\n",
    "    bpe_codes[best] = i\n",
    "    bpe_codes_reverse[best[0] + best[1]] = best\n",
    "\n",
    "    print(\"New merge: {}\".format(best))\n",
    "    print(\"Vocabulary: {}\".format(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a8bf6",
   "metadata": {},
   "source": [
    "## 2. Pretrained Tokenizer (WordPiece)\n",
    "\n",
    "트랜스포머스는 사전 훈련된 언어모델을 활용하기 위하여 언어모델 구축에 사용된 토크나이저를 로딩하는 **AutoTokenizer** 클래스를 \n",
    "제공합니다.  \n",
    "이 클래스의 **.from_pretrained()** 메소드를 이용하여 모델ID나 로컬 파일 경로로 호출하면 됩니다.  \n",
    "대표적인 Sub-word Tokenizer 방식으로 WordPiece 계열의 **BERT Tokenizer**를 이용하여 실습해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38a996c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'flies', 'like', 'an', 'arrow', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 'distilbert-base-uncased' 모델에 사용된 토크나이저를 불러옵니다.\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "tokenizer.tokenize(\"Time flies like an arrow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f15c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "512\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0cd1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', '##izing', 'a', 'text', 'is', 'splitting', 'it', 'into', 'words', 'or', 'sub', '##words', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tokenizing a text is splitting it into words or subwords.\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# tokenizer를 이용하여 sentence를 토크나이즈합니다.\n",
    "tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1a6a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19204, 6026, 1037, 3793, 2003, 14541, 2009, 2046, 2616, 2030, 4942, 22104, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence)\n",
    "\n",
    "encoded_sentence = inputs[\"input_ids\"]\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8b2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing a text is splitting it into words or subwords. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# tokenizer를 이용하여 encoded_sentence를 문장으로 복원합니다.\n",
    "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23df760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2051, 10029, 2066, 2019, 8612, 1012, 102, 0, 0, 0, 0, 0], [101, 2023, 6251, 2003, 2012, 2560, 2936, 2084, 1996, 6251, 1037, 1012, 102]]\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"Time flies like an arrow.\"\n",
    "sentence_b = \"This sentence is at least longer than the sentence A.\"\n",
    "\n",
    "padded_sentence = tokenizer([sentence_a, sentence_b], padding=True)\n",
    "print(padded_sentence[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa504de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentence[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a7e05",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Training\n",
    "\n",
    "특정 언어 모델이 당신이 원하는 언어를 제대로 지원하지 않거나, 현재 보유한 코퍼스가 언어 모델과 특성이 다른 경우, 해당 코퍼스에 적합하게 적응된 토크나이저를 이용하여 모델을 새롭게 학습하기를 원할 수도 있습니다. 이를 위해서는 그 데이터셋에 대한 새로운 토크나이저를 학습해야 합니다. 예로, 파이썬 코드를 토큰화 하기 위해 BPE 토크나이저(GPT2-XL)를 재훈련해 보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5ed902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"gpt2-xl\"\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98898a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Size: {len(old_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db7540",
   "metadata": {},
   "source": [
    "Python Code 이용하여 GPT2-XL 토크나이저를 테스트해 보겠습니다.  \n",
    "'Ġ', 'Ċ' 와 같은 문자가 있는 것은 GPT-2 토크나이저가 유니코드 문자가 아닌 바이트 단위로 동작하기 때문입니다.\n",
    "('Ġ': Space, 'Ċ': Line Feed,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2122b192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d58c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword `async` does not exist in Vocabulary!\n",
      "Keyword `await` does not exist in Vocabulary!\n",
      "Keyword `elif` does not exist in Vocabulary!\n",
      "Keyword `finally` does not exist in Vocabulary!\n",
      "Keyword `nonlocal` does not exist in Vocabulary!\n",
      "Keyword `yield` does not exist in Vocabulary!\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in old_tokenizer.vocab:\n",
    "        print(f'Keyword `{keyw}` does not exist in Vocabulary!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f938d5",
   "metadata": {},
   "source": [
    "파이썬 코드 데이터(code_search_net/python)을 이용하여 토크나이저를 새로 학습하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd391a",
   "metadata": {},
   "source": [
    "데이터셋 **\"code search_net\"** 는 CodeSearchNet Challenge를 위해 구축된 것으로, GitHub 상의 오픈소스 코드를 포함하고 있으며 파이썬을 포함 몇개 프로그래밍 언어로 구분되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc0c4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The repository for code_search_net contains custom code which must be executed to correctly load the dataset. \n",
    "You can inspect the repository content at code_search_net.py\n",
    "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f10c2cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24e44212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _initialize_monitoring_services_queue(self, chain_state: ChainState):\n",
      "        \"\"\"Send the monitoring requests for all current balance proofs.\n",
      "\n",
      "        Note:\n",
      "            The node must always send the *received* balance proof to the\n",
      "            monitoring service, *before* sending its own locked transfer\n",
      "            forward. If the monitoring service is updated after, then the\n",
      "            following can happen:\n",
      "\n",
      "            For a transfer A-B-C where this node is B\n",
      "\n",
      "            - B receives T1 from A and processes it\n",
      "            - B forwards its T2 to C\n",
      "            * B crashes (the monitoring service is not updated)\n",
      "\n",
      "            For the above scenario, the monitoring service would not have the\n",
      "            latest balance proof received by B from A available with the lock\n",
      "            for T1, but C would. If the channel B-C is closed and B does not\n",
      "            come back online in time, the funds for the lock L1 can be lost.\n",
      "\n",
      "            During restarts the rationale from above has to be replicated.\n",
      "            Because the initialization code *is not* the same as the event\n",
      "            handler. This means the balance proof updates must be done prior to\n",
      "            the processing of the message queues.\n",
      "        \"\"\"\n",
      "        msg = (\n",
      "            'Transport was started before the monitoring service queue was updated. '\n",
      "            'This can lead to safety issue. node:{self!r}'\n",
      "        )\n",
      "        assert not self.transport, msg\n",
      "\n",
      "        msg = (\n",
      "            'The node state was not yet recovered, cant read balance proofs. node:{self!r}'\n",
      "        )\n",
      "        assert self.wal, msg\n",
      "\n",
      "        current_balance_proofs = views.detect_balance_proof_change(\n",
      "            old_state=ChainState(\n",
      "                pseudo_random_generator=chain_state.pseudo_random_generator,\n",
      "                block_number=GENESIS_BLOCK_NUMBER,\n",
      "                block_hash=constants.EMPTY_HASH,\n",
      "                our_address=chain_state.our_address,\n",
      "                chain_id=chain_state.chain_id,\n",
      "            ),\n",
      "            current_state=chain_state,\n",
      "        )\n",
      "        for balance_proof in current_balance_proofs:\n",
      "            update_services_from_balance_proof(self, chain_state, balance_proof)\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c1e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a6e6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2dcdd",
   "metadata": {},
   "source": [
    "기존 토크나이저와 동일한 특성을 가진 새로운 토크나이저를 학습하기 위해 AutoTokenizer.train_new_from_iterator()를 사용하겠습니다.\n",
    "\n",
    "- 목표하는 Vocabulary Size 설정\n",
    "- 토크나이저 학습을 위해 입력 문자열을 공급하는 Iterator 설정\n",
    "- train_new_from_iterator() 메소드 호출\n",
    "\n",
    "토크나이저 학습에 약 2~3분 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53485219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# training_corpus를 이용하여 토크나이저를 학습합니다.\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b40b1c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = new_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c747073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c2cbb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword `nonlocal` does not exist in Vocabulary!\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'Keyword `{keyw}` does not exist in Vocabulary!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a1b02",
   "metadata": {},
   "source": [
    "- Ref. https://huggingface.co/learn/nlp-course/chapter6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
