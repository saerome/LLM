{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b336dd04",
   "metadata": {},
   "source": [
    "# Llama-3-8B-Instruct Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c53b44",
   "metadata": {},
   "source": [
    "Meta Llama-3-8B_Instruct 모델을 이용하여 다양한 태스크를 테스트해 보고, SQuAD Evaluation Dataset을 이용하여 MRC(Machine Reading Comprehension) 성능을 테스트해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f7e3d",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78120235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b651a",
   "metadata": {},
   "source": [
    "## 1. Llama-3-8B-Instruct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680231eb",
   "metadata": {},
   "source": [
    "Meta Llama-3-8B-Instruct 모델을 가져오도록 하겠습니다.  \n",
    "모델을 가져오기 위해서는 **Hugging Face Access Token**이 필요하며, Llama 모델 사용에 대한 사전 신청이 필요합니다.  \n",
    "(https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16, # bfloat16\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741642d",
   "metadata": {},
   "source": [
    "모델 접근 권한이 어려운 경우 아래의 로컬 디렉토리(Group Volume)에 있는 모델을 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c3f96de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b933e85bedff4bd9a1c695df20899f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_ckpt = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_ckpt = \"/group-volume/sr_edu/AI-Application-Specialist/LLM/model/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.float16, # bfloat16\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3295c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_message, user_message):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "        \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    \n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717ed99",
   "metadata": {},
   "source": [
    "## 2. Llama-3-8B-Instruct Model Test (ChatBot, Translation, Coding, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc75c0",
   "metadata": {},
   "source": [
    "**generate_response( )** 함수를 이용하여 다양한 NLP Task 들에 대해 테스트해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bc9575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poem about summer:\n",
      "\n",
      "Summer's warmth descends upon the land\n",
      "A gentle breeze that whispers sweet commands\n",
      "The sun beats down, a fiery glow\n",
      "As petals bloom, and all around, it grows\n",
      "\n",
      "The scent of ripened fruits fills the air\n",
      "As children laugh, without a single care\n",
      "Their shouts and giggles echo through the day\n",
      "As they chase fireflies, in a joyful sway\n",
      "\n",
      "The world is full of vibrant hues and sounds\n",
      "As nature awakens from its winter bounds\n",
      "The trees regain their verdant, emerald sheen\n",
      "And wildflowers bloom, a colorful dream\n",
      "\n",
      "The nights are long, the stars shine bright and clear\n",
      "As crickets serenade, without a fear\n",
      "The world is full of magic, wild and free\n",
      "In summer's warmth, we find ecstasy\n",
      "\n",
      "So let us bask in summer's radiant glow\n",
      "And let our spirits soar, as the days grow slow\n",
      "For in this season of sunshine and delight\n",
      "We find our hearts filled with joy, and our souls alight.\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 다양한 NLP Task를 테스트해 보세요.\n",
    "response = generate_response(system_message=\"\",\n",
    "                             user_message=\"여름을 주제로 시를 작성해줘.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd159128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시간은 화살과 같이 날아간다.\n",
      "\n",
      "(Note: This is a common idiomatic expression in English, and the translation is also an idiomatic expression in Korean. The phrase \"like an arrow\" is used to convey the idea that time passes quickly and swiftly, just like an arrow flying through the air.)\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 다양한 NLP Task를 테스트해 보세요.\n",
    "response = generate_response(system_message=\"\",\n",
    "                             user_message=\"다음 문장을 한국어로 번역해줘, Time flies like an arrow.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2caa11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Python implementation of the QuickSort algorithm:\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[0]\n",
      "    less = [x for x in arr[1:] if x <= pivot]\n",
      "    greater = [x for x in arr[1:] if x > pivot]\n",
      "    return quicksort(less) + [pivot] + quicksort(greater)\n",
      "```\n",
      "Here's an explanation of how the algorithm works:\n",
      "\n",
      "1. If the length of the input array is 0 or 1, return the original array (since it's already sorted).\n",
      "2. Choose the first element of the array as the pivot.\n",
      "3. Create two lists: `less` and `greater`. `less` contains all elements in the array that are less than or equal to the pivot, and `greater` contains all elements that are greater than the pivot.\n",
      "4. Recursively call the `quicksort` function on `less` and `greater`.\n",
      "5. Concatenate the results of the recursive calls, with the pivot element in its final position.\n",
      "\n",
      "Here's an example usage:\n",
      "```python\n",
      "arr = [5, 2, 8, 3, 1, 6\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# 다양한 NLP Task를 테스트해 보세요.\n",
    "response = generate_response(system_message=\"\",\n",
    "                             user_message=\"퀵소트 파이썬 알고리즘 작성해줘.\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c5df0",
   "metadata": {},
   "source": [
    "## 3. SQuAD DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1692f",
   "metadata": {},
   "source": [
    "**SQuAD** Dataset을 이용하여 MRC(Machine Reading Comprehension) 성능평가를 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dac18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('/group-volume/sr_edu/AI-Application-Specialist/LLM/dataset/squad')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea595db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d709c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset.set_format(type=\"pandas\")\n",
    "df = dataset[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2653c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49586345",
   "metadata": {},
   "source": [
    "Context, Question을 이용하여 LLM을 위한 Prompt Format으로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14311047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_question(data):\n",
    "    return f\"Context: {data['context']}\\nQuestion: {data['question']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e826340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n"
     ]
    }
   ],
   "source": [
    "print(format_context_question(dataset[\"validation\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "842a4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2958a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\\nQuestion: Which NFL team represented the AFC at Super Bowl 50?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_prompt = format_context_question(dataset[\"validation\"][0])\n",
    "question_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ca6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Denver Broncos represented the AFC at Super Bowl 50.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(system_message=\"너는 컨텍스트에 기반해서 질문에 답변을 하는 챗봇이야. 답변만 간결하게 영어로 작성해줘\",\n",
    "                             user_message=question_prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2511cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denver Broncos', 'Denver Broncos', 'Denver Broncos']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_text = dataset[\"validation\"][0][\"answers\"][\"text\"]\n",
    "ground_truth_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85baa8bc",
   "metadata": {},
   "source": [
    "## 4. SQuAD Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a83e54",
   "metadata": {},
   "source": [
    "EM(Exact Match)-Score, F1-Score 값을 계산합니다. 특히 F1-Score는 Ground Truth 들과 일치하는 토큰 비율을 평가하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2582a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"평가를 위하여 의미없는 Article, White Space, Punctuation 기호를 삭제합니다. \"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # Prediction 또는 Truth 값이 0 일 경우에는 두 값이 일치하면 f1 = 1 그렇지 않으면 0\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # 공통된 토큰이 없으면 f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4ca4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Prediction: The Denver Broncos represented the AFC at Super Bowl 50.\n",
      "True Answers: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "EM: 0 \t F1: 0.4\n"
     ]
    }
   ],
   "source": [
    "question = dataset[\"validation\"][0][\"question\"]\n",
    "prediction = response\n",
    "gold_answers = list(ground_truth_text)\n",
    "\n",
    "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
    "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"True Answers: {gold_answers}\")\n",
    "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b52d537",
   "metadata": {},
   "source": [
    "## 5. LLM Based Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75891e13",
   "metadata": {},
   "source": [
    "LLM을 활용하여 번역 성능을 자동으로 평가하는 실습을 진행하겠습니다. (LLM as a Judge)  \n",
    "System Message로 시스템의 역할을 정의하고, 정량적인 번역평가 가이드와 함께 평가 결과의 출력 형태를 제시합니다.   \n",
    "User Message로는 Original Sentence, Machine Translation, Reference Translation을 각각 제시하고 가이드에 따른 평가를 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3a6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습] 다음 코드를 완성하세요!!\n",
    "# LLM Based Evaluation을 위한 System Message와 User Message를 작성해 보세요.\n",
    "system_message = \"\"\"\n",
    "You are an expert in machine translation evaluation. \n",
    "Your task is to assess the translation quality of a given sentence compared to a reference translation. \n",
    "You will compare two translations, provide detailed feedback on the differences, and rate the translation quality on a scale from 1 to 5, where:\n",
    "1 = Poor (significant errors in meaning and grammar)\n",
    "2 = Fair (some errors in meaning or grammar)\n",
    "3 = Good (minor issues, but overall understandable)\n",
    "4 = Very good (accurate translation with minor stylistic differences)\n",
    "5 = Excellent (perfect translation)\n",
    "\n",
    "Provide your evaluation, including:\n",
    "1. A brief description of the errors (if any) in terms of meaning or grammar.\n",
    "2. A score from 1 to 5.\n",
    "3. A suggestion to improve the translation if necessary.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "Original Sentence (English): \"The quick brown fox jumps over the lazy dog.\"\n",
    "Machine Translation (French): \"Le rapide renard brun saute par-dessus le chien paresseux.\"\n",
    "Reference Translation (French): \"Le renard brun rapide saute par-dessus le chien endormi.\"\n",
    "\n",
    "Evaluate the translation quality based on the given input.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5ec99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\n",
      "The machine translation \"Le rapide renard brun saute par-dessus le chien paresseux\" has some minor issues compared to the reference translation \"Le renard brun rapide saute par-dessus le chien endormi\". The main difference is the word order, with the machine translation placing the adjective \"rapide\" after the noun \"renard\", whereas the reference translation places it before. This is a minor stylistic difference, but it does affect the sentence's overall flow.\n",
      "\n",
      "Additionally, the machine translation uses the word \"paresseux\" (lazy) instead of the more accurate \"endormi\" (asleep), which changes the meaning of the sentence slightly. The original sentence describes the dog as being asleep, whereas the machine translation implies it is simply lazy.\n",
      "\n",
      "Description of errors: Minor issues with word order and inaccurate adjective placement, and a small change in meaning due to the use of a different adjective.\n",
      "\n",
      "Score: 4 (Very good)\n",
      "\n",
      "Suggestion to improve the translation: The machine translation is close, but a slight improvement could be made by adopting the word order of the reference translation and using the more accurate adjective \"endormi\" to maintain the original meaning. This would result in a more accurate and natural-s\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(system_message=system_message, user_message=user_message)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
