{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae7495",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning (gemma-2b + LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a9269",
   "metadata": {},
   "source": [
    "공개된 대규모 언어모델 **\"gemma-2b\"** 을 Foundation Model로 하고, LoRA 기법을 적용하여 효과적으로 파인튜닝하는 실습을 진행해 보겠습니다.  \n",
    "- Task: Causal Language Model (Instruct Fine-Tuning)\n",
    "- Foundation Model: “gemma-2b”\n",
    "- Dataset: “KorAlpaca”\n",
    "- Trainer: Huggingface PEFT/LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16301bf",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82f9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318fe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635bdf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q --user transformers==4.38.2\n",
    "# !pip install -q --user datasets==2.18.0\n",
    "# !pip install -q --user peft==0.9.0\n",
    "# !pip install -q --user trl==0.7.11\n",
    "# !pip install -q --user accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244d2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ddddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 22 13:57:25 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:04:00.0 Off |                  Off |\n",
      "| N/A   33C    P8    10W / 250W |     10MiB / 24451MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36021449",
   "metadata": {},
   "source": [
    "## 1. DataSet: KoAlpaca v1.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bf274",
   "metadata": {},
   "source": [
    "질문을 전달하면 답변하는 형태로 대규모 언어모델을 파인튜닝하기 위한 학습 데이터셋을 준비하겠습니다.  \n",
    "데이터셋은 **Instruction** (지시사항)과 **Response** (출력)의 쌍으로 구성되어 있습니다.  \n",
    "KoAlpaca 데이터셋은 지식iN 기반의 질문-답변 데이터셋이며, Huggingface Dataset 에 공개되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbd7a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 21155\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"beomi/KoAlpaca-v1.1a\")\n",
    "dataset = load_dataset(\"/group-volume/sr_edu/AI-Application-Specialist/LLM/dataset/KoAlpaca-v1.1a\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b71ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?',\n",
       " 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.',\n",
       " 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c79aac",
   "metadata": {},
   "source": [
    "파인튜닝하기 위하여 학습 데이터를 다음과 같은 Instruction Format 형태로 변환을 합니다.  \n",
    "```\n",
    "### Question:\n",
    "\n",
    "### Response: <eos>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35197504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        prompt = f\"### Question: {example['instruction'][i]}\\n\\n### Response: {example['output'][i]}<eos>\"\n",
    "        output_texts.append(prompt)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e689df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: 양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\n",
      "\n",
      "### Response: 양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \n",
      "\n",
      "식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\n",
      "\n",
      " 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \n",
      "\n",
      "고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.<eos>\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "print(generate_prompt(train_data[:1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd93a5",
   "metadata": {},
   "source": [
    "## 2. Foundation Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b1c8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b9400",
   "metadata": {},
   "source": [
    "Foundation LLM 으로 \"google/gemma-2b\" 모델을 로딩합니다.  \n",
    "- Lightweight, state-of-the-art open models\n",
    "- Text-to-text, decoder-only LLM (English)\n",
    "- Instruction following & Multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51fc882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ac749958c4d3cb03f9c74c4aae8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BASE_MODEL = \"google/gemma-2b\"\n",
    "BASE_MODEL = \"/group-volume/sr_edu/AI-Application-Specialist/LLM/model/gemma-2b\"\n",
    "\n",
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# 사전 학습된 'google/gemma-2b' 모델과 토크나이저를 가져옵니다.\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, add_special_tokens=True)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f33606",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"건강하게 살기 위한 세 가지 방법은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651c8df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. <strong>운동</strong>\n",
      "2. <strong>식사</strong>\n",
      "3. <strong>정신적 안녕감</strong>\n",
      "\n",
      "이 중에서도 운동을 가장 중요시하는 이유는 무엇일까요?\n",
      "\n",
      "* 운동은 체력과 건강을 향상시키고, 심장질환이나 고혈압 등의 질병을 예방한다.\n",
      "* 운동은 정신적인 안녕감을 높여준다.\n",
      "* 운동은 신체적 안녕감을 높여준다.\n",
      "\n",
      "그러므로, 우리가 건강한 생활을 하기 위해서는 운동을 하는 것이 매우 중요하다고 할 수 있겠습니다.\n",
      "\n",
      "<h2>운동에 대한 이해</h2>\n",
      "\n",
      "우선, 운동이란 무엇인지 알아보도록 하겠습니다.\n",
      "\n",
      "<h3>무엇을 의미하는 ‘운동’</h3>\n",
      "\n",
      "‘운동’이라는 단어는 여러 가지 의미를 지니고 있습니다.\n",
      "\n",
      "* 운동은 사람이 어떤 일을 하면서 그 일을 몸으로 움직이는 것을 말합니다.\n",
      "* 운동은 사람이 몸을 활용하여 일을 하거나, 힘든 일을 해결하기 위해 몸을 사용하는 것을 말합니다.\n",
      "* 운동은 사람이 몸을 활용하여 일을 하거나, 힘든 일을 해결하기 위해 몸을 사용하는 것을 말합니다.\n",
      "\n",
      "위와 같은 의미로 사용되고 있는데, 이 중에서도 <strong>“자연스럽게”</strong> 하는 의미를 지닌 운동은 다음과 같습니다.\n",
      "\n",
      "<blockquote>자연스럽게 = 자연스럽게 하는 것은 자연스럽게 하는 것</blockquote>\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1451b3",
   "metadata": {},
   "source": [
    "## 3. PEFT LoRA Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68037fe",
   "metadata": {},
   "source": [
    "LoraConfig 함수를 통해 파인튜닝을 위한 주요 LoRA 파라미터를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e7dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# LoRA 파인튜닝을 위한 Config를 설정합니다. r, lora_alpha, lora_dropout, target_modules, task_type\n",
    "lora_config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f303c8d",
   "metadata": {},
   "source": [
    "## 4. Model Training (PEFT LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c820b",
   "metadata": {},
   "source": [
    "SFT Trainer 관련 필요한 학습 파라미터 설정한 후 PEFT LoRA 기반으로 파인튜닝을 위한 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9131ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# [실습] 다음 코드를 완성하세요!! \n",
    "# 파인튜닝을 위한 Trainer 설정을 합니다. (model, train_dataset, max_seq_length, args, peft_config, generate_func) \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "        num_train_epochs = 1,\n",
    "        max_steps=600, # 3000\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_hf\",\n",
    "        warmup_steps=0.03,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=200,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_prefetch_factor=2,\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=generate_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c6a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177a2ca",
   "metadata": {},
   "source": [
    "LoRA 모델을 통해 학습 가능한 파라미터수는 전체 모델 파라미터(2,513,526,784)의 0.5% 수준입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091727e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1382400 || all params: 2507554816 || trainable%: 0.05512940300165307\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cb190",
   "metadata": {},
   "source": [
    "600 스텝 학습에 25분 정도 소요되니, TrainingArguments에서 **`max_steps`** 설정하실 때 참고하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e363f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 21:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.845600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /group-volume/sr_edu/AI-Application-Specialist/LLM/model/gemma-2b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=1.8655912780761719, metrics={'train_runtime': 1271.3819, 'train_samples_per_second': 1.888, 'train_steps_per_second': 0.472, 'total_flos': 8353496029790208.0, 'train_loss': 1.8655912780761719, 'epoch': 0.11})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac41c7",
   "metadata": {},
   "source": [
    "학습된 LoRA Adapter 모델을 Local Directory 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADAPTER_MODEL = \"lora_adapter\"\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b2a4c5",
   "metadata": {},
   "source": [
    "LoRA 학습된 weight을 원래 gemma-2b 모델과 합쳐 하나의 모델로 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55647df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "# model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model.load_adapter(ADAPTER_MODEL)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('gemma-2b-peft')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82132bc6",
   "metadata": {},
   "source": [
    "## 5. Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda5add",
   "metadata": {},
   "source": [
    "파인튜닝된 모델을 이용하여 Generation 해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "815131ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_finetuned = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c426e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"건강하게 살기 위한 세 가지 방법은?\"\n",
    "formatted_prompt = f\"### Question: {prompt}\\n\\n### Response:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2113616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 운동을 하며, 2) 음식물의 양과 질을 관리하며, 3) 스트레스를 줄이는 것.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe_finetuned(\n",
    "    formatted_prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(formatted_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b7c57",
   "metadata": {},
   "source": [
    "- Ref. https://huggingface.co/docs/peft/main/en/task_guides/token-classification-lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
