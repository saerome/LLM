{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae7495",
   "metadata": {},
   "source": [
    "## LLM Fine-Tuning (polyglot-ko-1.3b + LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a9269",
   "metadata": {},
   "source": [
    "공개된 대규모 언어모델 **\"polyglot-ko-1.3b\"** 을 Foundation Model로 하여 PEFT LoRA 방법을 이용하여 파인튜닝하는 실습을 진행해 보겠습니다.  \n",
    "- Task: Causal Language Model (Instruct Fine-Tuning)\n",
    "- Foundation Model: “polyglot-ko-1.3b”\n",
    "- Dataset: “KorAlpaca”\n",
    "- Trainer: Huggingface PEFT/LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16301bf",
   "metadata": {},
   "source": [
    "### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "\n",
    "proxies = {\n",
    "'http': '75.17.107.42:8080',\n",
    "'https': '75.17.107.42:8080'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fe489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Suwon 설정 필요\n",
    "import ssl\n",
    "\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "   ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user transformers, datasets, peft, accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36021449",
   "metadata": {},
   "source": [
    "### 1. DataSet: KoAlpaca v1.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bf274",
   "metadata": {},
   "source": [
    "질문을 전달하면 답변하는 형태로 대규모 언어모델을 파인튜닝하기 위해, 학습 데이터셋을 준비하겠습니다.  \n",
    "데이터셋은 **Instruction**(지시사항)과 **Output**(출력)의 쌍으로 구성되어 있습니다.  \n",
    "KoAlpaca 데이터셋은 지식iN 기반의 질문-답변 데이터셋이며, https://huggingface.co/datasets/beomi/KoAlpaca-v1.1a 에 공개되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"beomi/KoAlpaca-v1.1a\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8efec",
   "metadata": {},
   "source": [
    "파인튜닝을 위하여 \"### 질문: ..... \\n\\n ### 답변: ..... <|endoftext|>\" 형태로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(\n",
    "    lambda x: {'text': f\"### 질문: {x['instruction']}\\n\\n### 답변: {x['output']}<|endoftext|>\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90137fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd93a5",
   "metadata": {},
   "source": [
    "### 2. Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b9400",
   "metadata": {},
   "source": [
    "Foundation LLM \"EleutherAI/polyglot-ko-1.3b\" 모델을 로딩합니다.  \n",
    "- Large-scale Korean Autoregressive LM\n",
    "- Trained on 863 GB (213 billion tokens)\n",
    "- GPT-NeoX framework\n",
    "- Evaluation on 5 downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec65614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "\n",
    "# 다음 코드를 완성하세요!! (사전학습 모델에 사용된 Tokenizer 가져오기)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 다음 코드를 완성하세요!! (사전학습 모델을 기반으로 CausalLM 구성)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc67e047",
   "metadata": {},
   "source": [
    "Polyglot Model 학습에 사용된 토크나이저를 이용하여 파인튜닝 데이터를 Preprocessing 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1451b3",
   "metadata": {},
   "source": [
    "### 3. PEFT: LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68037fe",
   "metadata": {},
   "source": [
    "LoraConfig 함수를 통해 주요 파인튜닝 파라미터를 설정하고, PEFT LoRA 파인튜닝이 가능한 형태로 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c6a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177a2ca",
   "metadata": {},
   "source": [
    "LoRA 모델을 통해 학습 가능한 파라미터수(1,572,864)는 전체 모델 파라미터(1,333,383,168)의 0.1% 수준입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091727e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 다음 코드를 완성하세요!! (LoraConfig의 r, lora_alpha 값 설정)\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f303c8d",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7ffca",
   "metadata": {},
   "source": [
    "Huggingface의 Transformer Trainer 기반으로 PEFT LoRA 파인튜닝을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d403ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7de879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 다음 코드를 완성하세요!! (Trainer 설정: batch_size, max_steps)\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=100, ## 초소량만 학습: 100 step에 약 5분정도 걸립니다.\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    "    # DataCollatorForLanguageModeling은 마스크 언어 모델링(MLM)과 인과적 언어 모델링(CLM)을 모두 지원합니다. \n",
    "    # 기본적으로 MLM용 데이터를 제공하지만 mlm=False 인수를 설정하여 CLM으로 전환할 수 있습니다:\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7b441",
   "metadata": {},
   "source": [
    "TextGenerationPipeline 함수를 이용하여 질문에 대한 답변을 생성해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # evaluation mode\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "text_generation_pipeline = TextGenerationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 코드를 완성하세요!! (문장 생성에 필요한 파라미터 설정: temperature, top_k, do_sample, no_repeat_ngram_size)\n",
    "def generate_text(prompt, max_length=256, temperature=0.5):\n",
    "    prompt = f\"### 질문: {prompt}\\n\\n### 답변:\"\n",
    "    generated_sequences = text_generation_pipeline(\n",
    "        prompt,\n",
    "        top_k=5, # The number of predictions to return\n",
    "        num_return_sequences=1,\n",
    "        temperature=temperature,\n",
    "        no_repeat_ngram_size=6,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "        pad_token_id=2, # tokenizer.eos_token_id\n",
    "        max_length=max_length,\n",
    "    )\n",
    " \n",
    "    return generated_sequences[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('건강하게 살기 위한 세 가지 방법은?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
